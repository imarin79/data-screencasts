---
title: "The Office"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(schrute) #it includes the database theoffice.
theme_set(theme_light())

office_transcripts <- as_tibble(theoffice) %>%
  mutate(season = as.integer(season),
         episode = as.integer(episode)) %>%
  mutate(character = str_remove_all(character, '"')) %>% #here we remove the "" in all elements of column character. 
  mutate(name = str_to_lower(str_remove_all(episode_name, "\\.| \\(Part.*"))) #here we remove . and (Part.*

office_ratings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv') %>%
  mutate(name = str_to_lower(str_remove_all(title, "\\.| \\(Part.*|\\: Part.*"))) # here we remove the periods, Part.*, and : Part.*
```

```{r}
library(ggrepel)

office_ratings %>%
  group_by(season) %>%
  summarize(avg_rating = mean(imdb_rating)) %>%
  ggplot(aes(season, avg_rating)) +
  geom_line() +
  scale_x_continuous(breaks = 1:9)

office_ratings %>%
  mutate(title = fct_inorder(title), # here we order factor by the order in which they appear.
         episode_number = row_number()) %>% #create a new column with the row number.
  ggplot(aes(episode_number, imdb_rating)) +
  geom_line() +
  geom_smooth() +
  geom_point(aes(color = factor(season), size = total_votes)) + #here we label the points on the line based on the factor season. 
  geom_text(aes(label = title), check_overlap = TRUE, hjust = 1) +
  expand_limits(x = -10) +
  theme(panel.grid.major.x = element_blank(),
        legend.position = "none") +
  labs(x = "Episode number",
       y = "IMDB Rating",
       title = "Popularity of The Office episodes over time",
       subtitle = "Color represents season, size represents # of ratings")
```

```{r}

#This is to find out the most popular episode:
office_ratings %>%
  arrange(desc(imdb_rating)) %>%
  mutate(title = paste0(season, ".", episode, " ", title),
         title = fct_reorder(title, imdb_rating)) %>%
  head(20) %>%
  ggplot(aes(title, imdb_rating, color = factor(season), size = total_votes)) +
  geom_point() +
  coord_flip() +
  labs(color = "Season", #this is the legend.
       title = "Most popular episodes of The Office")

```

### Transcripts
```{r}

library(tidytext)

blacklist <- c("yeah", "hey", "uh", "gonna")
blacklist_characters <- c("Everyone", "All", "Both", "Guy", "Girl", "Group")

transcript_words <- office_transcripts %>%
  group_by(character) %>%
  filter(n() >= 100,
         n_distinct(episode_name) > 2) %>%
  ungroup() %>%
  select(-text_w_direction) %>% 
  unnest_tokens(word, text) %>% #so, here we separate each word of each element of the column text and make it a row
  anti_join(stop_words, by = "word") %>% #here we join both datasets by the column word.
  filter(!word %in% blacklist,
         !character %in% blacklist_characters)

character_tf_idf <- transcript_words %>%
  add_count(word) %>%
  filter(n >= 20) %>%
  count(word, character) %>%
  bind_tf_idf(word, character, n) %>% #here we calculate the term frequency (tf) and the inverse document frequency (idf).
  arrange(desc(tf_idf))

#We want to know the number fo times the character Dwight appeared.
office_transcripts %>% 
  count(character, sort =TRUE) %>% 
  filter(character == "Dwight")


```

```{r}
character_tf_idf %>%
  filter(character %in% c("Dwight", "Jim", "David Wallace", "Darryl", "Jan", "Holly")) %>%
  group_by(character) %>%
  # add_count() %>% 
  # distinct(character,n()) %>% #here we quantify the number of appearances of each character. 
  top_n(10, tf_idf) %>% #top ten of characters based on tf_idf
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, character)) %>% #here we reorder the column word by tf_idf within each character.
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  coord_flip() +
  scale_x_reordered() + #reorderign the x axis based on frequency.
  facet_wrap(~ character, scales = "free_y") +
  labs(x = "",
       y = "TF-IDF of character-word pairs")
```

```{r}
office_transcripts %>%
  count(character, sort = TRUE) %>%
  filter(character == "Dwight")
```

### Machine learning model

What affects popularity of an episode:

* Season/time
* Director
* Writer
* Lines per character

```{r}
#We are using here the original office_ratings dataset (uploaded at the top of the script)
  
ratings_summarized <- office_ratings %>%
  group_by(name) %>%
  summarize(imdb_rating = mean(imdb_rating)) #here we calculate the mean of imdb_rating by unique names

character_lines_ratings <- office_transcripts %>%
  filter(!character %in% blacklist_characters) %>% #filter the elements that are in blacklist but not in character column.
  count(character, name) %>%
  group_by(character) %>%
  filter(sum(n) >= 50,
         n() >= 5) %>%
  inner_join(ratings_summarized, by = "name")

character_lines_ratings %>%
  summarize(avg_rating = mean(imdb_rating),
            nb_episodes = n()) %>%
  arrange(desc(avg_rating)) %>%
  View()

```

```{r}

director_writer_features <- office_transcripts %>%
  distinct(name, director, writer) %>% # Since there are repeated elements in name, director, and writer, we pass the function distinct to show just rows in which each element of these three variables are shown just once.
  gather(type, value, director, writer) %>% # Here we put the columns director and writer from horizontal to perpendicular as type and value. The rest of the columns will stay the same. 
  separate_rows(value, sep = ";") %>% #here we separate those elements in value that have ;
  unite(feature, type, value, sep = ": ") %>% #here we unite or fuse the columns type and value separated by : and name it as feature. 
  group_by(feature) %>%
  filter(n() >= 3) %>% #we only include here those repeated 3 or more times. 
  mutate(value = 1) %>%
  ungroup()

character_line_features <- character_lines_ratings %>%
  ungroup() %>%
  transmute(name, feature = character, value = log2(n)) #here we changes and only keep these columns. 

season_features = office_ratings %>%
  distinct(name, season) %>%
  transmute(name, feature = paste("season:", season), value = 1) #to add these variables and drop existing ones. 

features <- bind_rows(director_writer_features,
                      character_line_features,
                      season_features) %>% #here we stack these datasets on top of each other. 
  semi_join(office_ratings, by = "name") %>%
  semi_join(office_transcripts, by = "name")

```

```{r}
#Here we create a sparce matrix:
episode_feature_matrix <- features %>%
  cast_sparse(name, feature, value) #this is a way to spread the data included in these three variables. 

ratings <- ratings_summarized$imdb_rating[match(rownames(episode_feature_matrix), ratings_summarized$name)]

library(glmnet)
library(broom)

#here we perform cross validation of a generalized linear model. In this case we use the episode_feature_matrix to predict the ratings:
mod <- cv.glmnet(episode_feature_matrix, ratings) #this is showing the log(lambda) to see how it changes relative to mean-squared error.

plot(mod)

tidy(mod$glmnet.fit) %>%
  filter(lambda == mod$lambda.min, #here we pick a lambda
         term != "(Intercept)") %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(term, estimate, fill = estimate > 0)) +
  geom_col() +
  coord_flip() +
  labs(y = "Estimated effect on the rating of an episode") +
  theme(legend.position = "none")

dev.off()

```
